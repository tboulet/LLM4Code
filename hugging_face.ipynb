{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Import tokenizers, create/import huggingface models\n",
    "\n",
    "In this part we learn how to import from the HF hub tokenizers and models.\n",
    "We then give an example of a simple inference pipeline : from a batch of textual sentences to a batch of sequence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Import a tokenizer from a specific checkpoint\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Convert text to model-input\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "tokenized_sentences = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(tokenized_sentences)\n",
    "\n",
    "# Decode tokens IDs back to a string\n",
    "decoded = tokenizer.decode(tokenized_sentences[\"input_ids\"][0])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n",
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Import a model from a specific checkpoint\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**tokenized_sentences)\n",
    "print(outputs.last_hidden_state.shape) # (B, T, hidden_size)\n",
    "\n",
    "# You can import a model for a different task but from the same checkpoint. AutoModel is a model : sentence --> hidden states (decoder)\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**tokenized_sentences)\n",
    "print(outputs.logits)\n",
    "print(outputs.logits.shape) # (B, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a transformer\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "print(f\"config: {config}\")\n",
    "\n",
    "# Building the model\n",
    "model = BertModel(config) # from the config (architecture) : random weights\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\") # from a checkpoint : pretrained weights\n",
    "# model = AutoModel.from_pretrained(\"bert-base-cased\") # from a checkpoint : pretrained weights (same as above)\n",
    "\n",
    "model.save_pretrained(\"models/my_bert_model\") # save the model (architecture + weights) to a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 7592, 1010, 1045, 1005, 1049, 1037, 2309, 6251,  102],\n",
      "        [ 101, 1998, 2178, 6251,  102,    0,    0,    0,    0,    0]])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21297/1726034789.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model_inputs = torch.tensor(inputs.input_ids) # convert the input_ids to a tensor\n"
     ]
    }
   ],
   "source": [
    "# Perform a forward pass :\n",
    "\n",
    "# 1. We have some input data\n",
    "sentences = [\n",
    "    \"Hello, I'm a single sentence\",\n",
    "    \"And another sentence\",\n",
    "]\n",
    "\n",
    "# 2. Tokenize the inputs : from a list of sentences to a list-matrix (thanks to padding) of token ids of shape (B, T_max)\n",
    "tokenized_sentences = tokenizer(\n",
    "    sentences, \n",
    "    padding=True,  # unsure shortest sentences are padded to the length of the longest sentence\n",
    "    truncation=True,   # truncate the sentences to the maximum length of the model \n",
    "    return_tensors=\"pt\", # return PyTorch tensors (?)\n",
    ")\n",
    "print(tokenized_sentences.input_ids)\n",
    "print(tokenized_sentences.input_ids.shape) # (B, T_max) with T_max = max length of the sentences of the batch\n",
    "\n",
    "# 3. Convert the input_ids to a pytorch tensor. This step is actually not necessary as the tokenizer can return tensors directly.\n",
    "import torch\n",
    "model_inputs = torch.tensor(tokenized_sentences.input_ids) # convert the input_ids to a tensor\n",
    "print(model_inputs.shape) # (B, T_max)\n",
    "\n",
    "# 4. Perform a forward pass of the model\n",
    "outputs = model(model_inputs)\n",
    "print(outputs.last_hidden_state.shape) # (B, T_max, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call x --> tokenizer(x, padding=True, truncation=True, return_tensors='pt') is a very powerfull method, because :\n",
    "- it can handle non-batched inputs (a single string, treated as a batch of size 1)\n",
    "- it padds and truncates the inputs so that it is treatable by the model\n",
    "- it can return the inputs as PyTorch tensors directly\n",
    "\n",
    "In a bit more low level, there is the following code generally used :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences : [\"Hello, I'm a single sentence\", 'And another sentence']\n",
      "Tokens Object (wrong concatenating way) : ['hello', ',', 'i', \"'\", 'm', 'a', 'single', 'sentence', 'and', 'another', 'sentence']\n",
      "Tokens Object (correct batching way) : [['hello', ',', 'i', \"'\", 'm', 'a', 'single', 'sentence'], ['and', 'another', 'sentence']]\n",
      "IDs of tokens: list of list of lens : [8, 3] with each list containing types : <class 'int'>\n",
      "Padding ID : 0\n",
      "Max Context Length : 512\n",
      "IDs of tokens padded and truncated : [[7592, 1010, 1045, 1005, 1049, 1037, 2309, 6251], [1998, 2178, 6251, 0, 0, 0, 0, 0]]\n",
      "Inputs model shape: torch.Size([2, 8])\n",
      "Attention mask : tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.]])\n",
      "Last hidden states shape: torch.Size([2, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Hello, I'm a single sentence\",\n",
    "    \"And another sentence\",\n",
    "]\n",
    "print(f\"Original sentences : {sentences}\")\n",
    "\n",
    "# Sentences to tokens IDs\n",
    "tokens = tokenizer.tokenize(sentences) # carefull, this will create a list of tokens of size sum(len(sentence) for sentence in sentences)...\n",
    "print(f\"Tokens Object (wrong concatenating way) : {tokens}\")\n",
    "\n",
    "tokens = [tokenizer.tokenize(sentence) for sentence in sentences] # ... instead of a list of list of tokens\n",
    "print(f\"Tokens Object (correct batching way) : {tokens}\")\n",
    "\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens] # convert the tokens to ids\n",
    "print(f\"IDs of tokens: list of list of lens : {[len(i) for i in ids]} with each list containing types : {type(ids[0][0])}\")\n",
    "\n",
    "# Padding and truncating\n",
    "padding_id = tokenizer.pad_token_id # get the padding token id\n",
    "max_context_length = model.config.max_position_embeddings # get the maximum length of the model\n",
    "print(f\"Padding ID : {padding_id}\")\n",
    "print(f\"Max Context Length : {max_context_length}\")\n",
    "T_max = max([len(i) for i in ids]) # get the maximum length of the sentences\n",
    "ids_padded = [i + [padding_id] * (T_max - len(i)) for i in ids] # pad the sentences\n",
    "ids_padded_truncated = [i[:max_context_length] for i in ids_padded] # truncate the sentences\n",
    "print(f\"IDs of tokens padded and truncated : {ids_padded_truncated}\")\n",
    "inputs_model = torch.tensor(ids_padded_truncated)\n",
    "print(f\"Inputs model shape: {inputs_model.shape}\") # (B, min(T_max, max_context_length))\n",
    "\n",
    "# Getting attention masks\n",
    "attention_mask = (inputs_model != padding_id).float() # create the attention mask\n",
    "print(f\"Attention mask : {attention_mask}\")\n",
    "\n",
    "# Infer hidden states\n",
    "outputs = model(inputs_model, attention_mask=attention_mask) # perform a forward pass\n",
    "print(f\"Last hidden states shape: {outputs.last_hidden_state.shape}\") # (B, T, hidden_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The attention mask\n",
    "\n",
    "The attention mask can be accessed through the returned dictionary of the tokenizer : ```tokenizer(sentences).attention_mask```. \n",
    "\n",
    "It is a tensor of 0s and 1s, with 1s indicating the tokens that should have an influence on the attention of any other word in the same sentence.\n",
    "\n",
    "For inference for example, without attention masking, [\"hello\", \"world\"] and [\"hello\", \"world\", \"[PAD]\"] would have different embeddings, because the padding token would have an influence on the attention of the other tokens. It has also a role to play in training (masking the future tokens in the case of autoregressive models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding tokens IDs\n",
    "\n",
    "This is the procedure to decode the token IDs back to strings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sentence 1 : hello, i'm a single sentence\n",
      "Decoded sentence 2 : and another sentence [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Decoded sentence 2 without special tokens : and another sentence\n",
      "Decoded sentence 1 : [CLS] hello, i'm a single sentence [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Decode the first sentence. We can see capital letters are removed.\n",
    "tokens_ids_sentence_1 = inputs_model[0, :]\n",
    "decoded_sentence_1 = tokenizer.decode(tokens_ids_sentence_1)\n",
    "print(f\"Decoded sentence 1 : {decoded_sentence_1}\")\n",
    "\n",
    "# Concerning sentence 2, we can see the apparition of padding tokens. \n",
    "tokens_ids_sentence_2 = inputs_model[1, :]\n",
    "decoded_sentence_2 = tokenizer.decode(tokens_ids_sentence_2)\n",
    "print(f\"Decoded sentence 2 : {decoded_sentence_2}\")\n",
    "\n",
    "# You can remove them using \"skip_special_tokens=True\"\n",
    "decoded_sentence_2 = tokenizer.decode(tokens_ids_sentence_2, skip_special_tokens=True)\n",
    "print(f\"Decoded sentence 2 without special tokens : {decoded_sentence_2}\")\n",
    "\n",
    "# If you use tokenizer(), because this high level method prepares the input for the model, it adds [CLS] (beginning of the sentence) and [SEP] (end of the sentence) tokens.\n",
    "ids = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "sentences_decoded = tokenizer.decode(ids[0])\n",
    "print(f\"Decoded sentence 1 : {sentences_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "output = model(**tokens)\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters : 125288025\n",
      "Number of not trainable parameters : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 5856.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters : 126774683\n",
      "Number of not trainable parameters : 0\n",
      "None\n",
      "Number of trainable parameters : 126774683\n",
      "Number of not trainable parameters : 0\n",
      "Number of trainable parameters : 2129051\n",
      "Number of not trainable parameters : 124645632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load adapters from the hub on top of a model\n",
    "\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "def count_parameters(model):\n",
    "    n_param_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    n_param_not_trainable = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    print(f\"Number of trainable parameters : {n_param_trainable}\")\n",
    "    print(f\"Number of not trainable parameters : {n_param_not_trainable}\")\n",
    "\n",
    "# 1. Load any model\n",
    "model = AutoAdapterModel.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "count_parameters(model)\n",
    "\n",
    "# 2. Load an adapter to the model (this will add the adapter to the model)\n",
    "adapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-imdb\")\n",
    "count_parameters(model) # add the adapter parameters\n",
    "\n",
    "# 3. Set the adapter as active \n",
    "model.active_adapters = adapter_name # for inference\n",
    "count_parameters(model)\n",
    "model.train_adapter(adapter_name) # for training\n",
    "count_parameters(model) # some parameters (all non-adapter and all non-last layer (probably)) are frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an adapter from scratch with PEFT\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    meta_learn_adapter=True,\n",
    "    meta_learn_adapter_lr=1e-3,\n",
    "    meta_learn_adapter_wd=1e-3,\n",
    "    meta_learn_adapter_max_steps=1000,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AdapterInfo(source='hf', adapter_id='BramVanroy/llama2-13b-ft-mc4_nl_cleaned_tiny', model_name=None, task=None, subtask=None, username='BramVanroy', adapter_config=None, sha1_checksum='0b6b07e68973136e4c29c2f0ee120546b3cb6c60'),\n",
       " AdapterInfo(source='hf', adapter_id='TheBloke/Llama-2-13B-Chat-Dutch-GPTQ', model_name=None, task=None, subtask=None, username='TheBloke', adapter_config=None, sha1_checksum='5f0b1b031f6b70b5c670b882d442f218c69bcbd6'),\n",
       " AdapterInfo(source='hf', adapter_id='TheBloke/Llama-2-13B-Chat-Dutch-GGUF', model_name=None, task=None, subtask=None, username='TheBloke', adapter_config=None, sha1_checksum='615b4ff967d510388e41a23f69d26ed0d5a6671c'),\n",
       " AdapterInfo(source='hf', adapter_id='TheBloke/Llama-2-13B-Chat-Dutch-AWQ', model_name=None, task=None, subtask=None, username='TheBloke', adapter_config=None, sha1_checksum='ddf1068fb40a4299e1823a9fbd85a8f0f07c044e'),\n",
       " AdapterInfo(source='hf', adapter_id='JakeTurner616/Adonalsium-Mistral-Adapters', model_name=None, task=None, subtask=None, username='JakeTurner616', adapter_config=None, sha1_checksum='6feb9a82448d78aa17ce7a5ef82b1c5cf76f1a8e'),\n",
       " AdapterInfo(source='hf', adapter_id='awels/maximusLLM-3b-128k-gguf', model_name=None, task=None, subtask=None, username='awels', adapter_config=None, sha1_checksum='60edd9aa59758a813908376aecdc12467c607521'),\n",
       " AdapterInfo(source='hf', adapter_id='awels/threadyLLM-3b-128k-gguf', model_name=None, task=None, subtask=None, username='awels', adapter_config=None, sha1_checksum='7e7072ca14b85161158f798e481031da554e3a1f'),\n",
       " AdapterInfo(source='hf', adapter_id='lamm-mit/stable-diffusion-3-medium-leaf-inspired', model_name=None, task=None, subtask=None, username='lamm-mit', adapter_config=None, sha1_checksum='055ded4528cb226ea68b767dacdc8acfbb4b8fed'),\n",
       " AdapterInfo(source='hf', adapter_id='awels/maximusLLM-14b-128k-gguf', model_name=None, task=None, subtask=None, username='awels', adapter_config=None, sha1_checksum='c4719b11ef0b56ebfc5b4e86391404c3a8c5004e'),\n",
       " AdapterInfo(source='hf', adapter_id='awels/threadyLLM-14b-128k-gguf', model_name=None, task=None, subtask=None, username='awels', adapter_config=None, sha1_checksum='6eab84f71c02e0a94e4796fd788742aca3fff2fc'),\n",
       " AdapterInfo(source='hf', adapter_id='awels/merlinLLM-14b-128k-gguf', model_name=None, task=None, subtask=None, username='awels', adapter_config=None, sha1_checksum='a5ec759cba48938737ed171c81adcbceffb1aac3'),\n",
       " AdapterInfo(source='hf', adapter_id='awels/merlinLLM-4b-128k', model_name=None, task=None, subtask=None, username='awels', adapter_config=None, sha1_checksum='60f13b987f7b1588fc7bec681f4107b28200783b'),\n",
       " AdapterInfo(source='hf', adapter_id='awels/maximusLLM-4b-128k', model_name=None, task=None, subtask=None, username='awels', adapter_config=None, sha1_checksum='bf13e289a68f956a8e9e421a3d3cc11b92b2cb27'),\n",
       " AdapterInfo(source='hf', adapter_id='SriSanth2345/LLAMA-3.2-3B-MathInstruct_LORA_SFT', model_name=None, task=None, subtask=None, username='SriSanth2345', adapter_config=None, sha1_checksum='b6b3e4ca816c1d9e1a8145f15ddb22dd3c373725')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from adapters import list_adapters\n",
    "\n",
    "# source can be \"ah\" (AdapterHub), \"hf\" (hf.co) or None (for both, default)\n",
    "adapter_infos = list_adapters()\n",
    "adapter_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_global",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
